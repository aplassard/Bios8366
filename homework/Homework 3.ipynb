{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer all questions and submit them either as an IPython notebook, LaTeX document, or Markdown document. Provide full answers for each question, including interpretation of the results. Each question is worth 25 points.\n",
    "\n",
    "This homework is due on Monday, November 30, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The `titanic.xls` spreadsheet in the `data` directory contains data regarding the passengers on the Titanic when it sank in 1912. A recent [Kaggle competition](http://www.kaggle.com/c/titanic-gettingStarted) was based on predicting survival for passengers based on the attributes in the passenger list. \n",
    "\n",
    "Use scikit-learn to build both a support vector classifier and a logistic regression model to predict survival on the Titanic. Use cross-validation to assess your models, and try to tune them to improve performance.\n",
    "\n",
    "Discuss the benefits and drawbacks of both approaches for application to such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The data in `prostate.data.txt` come from a study by Stamey et al. (1989), which examined the correlation between the level of prostate-specific antigen (`lpsa`) and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are log cancer volume (`lcavol`), log prostate weight (`lweight`), age, log of the amount of benign prostatic hyperplasia (`lbph`), seminal vesicle invasion (`svi`), log of capsular penetration (`lcp`), Gleason score (`gleason`), and percent of Gleason scores 4 or 5 (`pgg45`). \n",
    "\n",
    "1. Select (your choice) five competing 3-variable linear regression models, and compare them using AIC, five-fold and ten-fold cross-validation. Discuss the results.\n",
    "\n",
    "2. An alternative method for model assessment is to fit the models on a set of bootstrap samples, and then keep track of how well it predicts the original training set. If $\\hat{f}^b(x_i)$ is the predicted value at $x_i$, from the model fitted to the bth bootstrap dataset, such an estimate is:\n",
    "$$\\frac{1}{B} \\frac{1}{N} \\sum_{b=1}^B \\sum_{i=1}^N L(y_i,\\hat{f}^b(x_i)) $$\n",
    "However, because the bootstrap samples tend to contain many observations in common among the set of bootstrap samples, this estimate will tend to underestimate the true error rate. The so-called .632 estimator aleviates this bias by returning a weighted average of the training error (average loss over the training sample) and the leave-one-out (LOO) bootstrap error:\n",
    "$$\\hat{err}^{(.632)} = 0.368 \\, \\bar{err} + 0.632 \\, \\hat{err}^{(1)}$$\n",
    "where:\n",
    "$$\\bar{err} = \\frac{1}{N}\\sum_{i=1}^N L(y_i, \\hat{f}(x_i)) $$\n",
    "Repeat the assesment from part (1) using the .632 estimator, and compare the result to the other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "mat = pd.read_csv('../data/prostate.data.txt',sep='\\t')\n",
    "X = mat.copy()\n",
    "variables = ['lcavol','lweight','lbph','svi','lcp','gleason','pgg45']\n",
    "y = X.pop('lpsa')\n",
    "X = {x:X.pop(x) for x in variables}\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at AIC\n",
    "aic = lambda g: g.nobs * np.log((g.resid**2).sum()/g.nobs) + 2*len(g.beta)\n",
    "\n",
    "models = [\n",
    "    [1,1,0,1,0,0,0],\n",
    "    [1,0,1,0,0,0,1],\n",
    "    [0,1,0,0,1,1,0],\n",
    "    [0,0,0,0,1,1,1],\n",
    "    [0,1,0,1,0,1,0],\n",
    "]\n",
    "models = np.array(models)>0\n",
    "aics = []\n",
    "for i in range(models.shape[0]):\n",
    "    X_pred = X[X.columns[models[i,:]]]\n",
    "    g = pd.ols(y=y, x=X_pred)\n",
    "    aics.append(aic(g))\n",
    "print(aics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the second model is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "# Switching to scikit-learn since that's more comfortable for me\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "n_folds = 5\n",
    "kfold = cross_validation.KFold(X.shape[0],n_folds=n_folds,shuffle=True)\n",
    "scores = np.zeros([n_folds,models.shape[0]])\n",
    "n = 0\n",
    "for train,test in kfold:\n",
    "    for i in range(models.shape[0]):\n",
    "        X_train = X[train,:]\n",
    "        X_train = X_train[:,models[i,:]]\n",
    "        X_test = X[test,:]\n",
    "        X_test = X_test[:,models[i,:]]\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "        scores[n,i] = metrics.mean_squared_error(y_hat,y_test)\n",
    "    n += 1\n",
    "plt.xkcd()\n",
    "plt.boxplot(scores)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Model')\n",
    "plt.title('5 Fold Cross-Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "# Switching to scikit-learn since that's more comfortable for me\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "n_folds = 10\n",
    "kfold = cross_validation.KFold(X.shape[0],n_folds=n_folds,shuffle=True)\n",
    "scores = np.zeros([n_folds,models.shape[0]])\n",
    "n = 0\n",
    "for train,test in kfold:\n",
    "    for i in range(models.shape[0]):\n",
    "        X_train = X[train,:]\n",
    "        X_train = X_train[:,models[i,:]]\n",
    "        X_test = X[test,:]\n",
    "        X_test = X_test[:,models[i,:]]\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "        scores[n,i] = metrics.mean_squared_error(y_hat,y_test)\n",
    "    n += 1\n",
    "plt.xkcd()\n",
    "plt.boxplot(scores)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Model')\n",
    "plt.title('10 Fold Cross-Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_bootstrap_sample(n,rng_state=None):\n",
    "    held_out = np.ones(n,dtype=bool)\n",
    "    np.random.RandomState(rng_state)\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        k = np.random.randint(n)\n",
    "        samples.append(k)\n",
    "        held_out[k] = False\n",
    "    samples = np.array(samples)\n",
    "    return samples,held_out\n",
    "    \n",
    "n_folds = 100\n",
    "scores = np.zeros([n_folds,models.shape[0]])\n",
    "for i in range(n_folds):\n",
    "    samples,held_out = build_bootstrap_sample(X.shape[0])\n",
    "    y_train = y[samples]\n",
    "    y_test = y[held_out]\n",
    "    for j in range(models.shape[0]):\n",
    "        X_train = X[samples,:]\n",
    "        X_test = X[held_out,:]\n",
    "        X_train = X_train[:,models[j,:]]\n",
    "        X_test = X_test[:,models[j,:]]\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_train_hat = model.predict(X_train)\n",
    "        y_test_hat = model.predict(X_test)\n",
    "        err_bar = metrics.mean_squared_error(y_train,y_train_hat)\n",
    "        err_boot = metrics.mean_squared_error(y_test_hat,y_test)\n",
    "        err = 0.368*err_bar + 0.632*err_boot\n",
    "        scores[i,j] = err\n",
    "plt.xkcd()        \n",
    "plt.boxplot(scores)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Model')\n",
    "plt.title('Bootstrap error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Fit a series of random-forest classifiers to the very low birthweight infant data (`vlbw.csv`), to explore the sensitivity to the parameter `m`, the number of variables considered for splitting at each step. Plot both the out-of-bag error as well as the test error against a suitably-chosen range of values for `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Plassard\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:26: FutureWarning: the 'outtype' keyword is deprecated, use 'orient' instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a58cc0>,\n",
       " <matplotlib.lines.Line2D at 0x2a5e630>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeb9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_onehot(df, cols):\n",
    "    \"\"\"\n",
    "    One-hot encoding is applied to columns specified in a pandas DataFrame.\n",
    "    \n",
    "    Modified from: https://gist.github.com/kljensen/5452382\n",
    "    \n",
    "    Details:\n",
    "    \n",
    "    http://en.wikipedia.org/wiki/One-hot\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode\n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    vec_data = pd.DataFrame(vec.fit_transform(df[cols].to_dict(outtype='records')).toarray())\n",
    "    vec_data.columns = vec.get_feature_names()\n",
    "    vec_data.index = df.index\n",
    "    \n",
    "    df = df.drop(cols, axis=1)\n",
    "    df = df.join(vec_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "mat = pd.read_csv('../data/vlbw.csv',sep=',')\n",
    "X = mat.copy()\n",
    "y = X.pop('dead')\n",
    "_ = X.pop('Unnamed: 0')\n",
    "mask = np.zeros(X.shape[1],dtype=bool)\n",
    "mask[5] = True\n",
    "mask[8] = True\n",
    "mask[14] = True\n",
    "mask[20] = True\n",
    "mask[21] = True\n",
    "mask[22] = True\n",
    "mask[24] = True\n",
    "X = encode_onehot(X,[x for x in X.columns[mask]])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X[np.isnan(X)] = -1\n",
    "\n",
    "min_estimators = 2\n",
    "max_estimators = X.shape[1]\n",
    "n_folds = 10\n",
    "\n",
    "test_error = np.zeros((max_estimators-min_estimators+1,n_folds))\n",
    "oob_error = np.zeros((max_estimators-min_estimators+1,n_folds))\n",
    "\n",
    "kfold = KFold(X.shape[0],n_folds=n_folds,shuffle=True)\n",
    "\n",
    "n_fold = 0\n",
    "for train,test in kfold:\n",
    "    print(n_fold)\n",
    "    X_train = X[train,:]\n",
    "    X_test = X[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    n_est = 0\n",
    "    for i in range(min_estimators,max_estimators+1):\n",
    "        clf = RandomForestClassifier(max_features=i,oob_score=True,n_estimators=50,max_depth=10)\n",
    "        clf.fit(X_train,y_train)\n",
    "        oob_err = 1 - clf.oob_score\n",
    "        test_err = 1 - clf.score(X_test,y_test)\n",
    "        oob_error[n_est,n_fold] = oob_err\n",
    "        test_error[n_est,n_fold] = test_err\n",
    "        n_est += 1\n",
    "    n_fold += 1\n",
    "    \n",
    "plt.plot(np.arange(min_estimators,max_estimators+1),test_error.mean(axis=1),np.arange(min_estimators,max_estimators+1),oob_error.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(min_estimators,max_estimators+1),test_error.mean(axis=1),'b-',np.arange(min_estimators,max_estimators+1),oob_error.mean(axis=1),'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Question 4\n",
    "\n",
    "Use a grid search to optimize the number of estimators and max_depth for a Gradient Boosted Decision tree using the very low birthweight infant data. Plug this optimal ``max_depth`` into a *single* decision tree.  Does this single tree over-fit or under-fit the data? Repeat this for the Random Forest.  Construct a single decision tree using the ``max_depth`` which is optimal for the Random Forest.  Does this single tree over-fit or under-fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your work here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
